Dynamic stochastic general equilibrium modeling (abbreviated DSGE or sometimes SDGE or DGE) is a branch of applied general equilibrium theory that is influential in contemporary macroeconomics.[1] The DSGE methodology attempts to explain aggregate economic phenomena, such as economic growth, business cycles, and the effects of monetary and fiscal policy, on the basis of macroeconomic models derived from microeconomic principles.
While traditional macroeconometric forecasting models are vulnerable to the Lucas critique—that claims that the effects of an economic policy cannot be predicted using historical data from a period when that policy was not in place—microfounded models should not be, at least in theory. Further, since the microfoundations are based on the preferences of the decision-makers in the model, DSGE models feature a natural benchmark for evaluating the welfare effects of policy changes.[2][3]
Furthermore, as their name indicates, DSGE models are dynamic, studying how the economy evolves over time. They are also stochastic, taking into account the fact that the economy is affected by random shocks such as technological change, fluctuations in the price of oil, or changes in macroeconomic policy-making. This contrasts with the static models studied in Walrasian general equilibrium theory, applied general equilibrium models and some computable general equilibrium models.
For a coherent description of the macroeconomy, DSGE models must spell out the following economic 'ingredients'.
Traditional macroeconometric forecasting models used by central banks in the 1970s, and even today, estimated the dynamic correlations between prices and quantities in different sectors of the economy, and often included thousands of variables. Since DSGE models start from microeconomic principles of constrained decision-making, instead of just taking as given observed correlations, they are technically more difficult to solve and analyze. Therefore, they usually abstract from so many sectoral details, and include far fewer variables: just a few variables in theoretical DSGE papers, or on the order of a hundred variables in the experimental DSGE forecasting models now being constructed by central banks. What DSGE models give up in sectoral detail, they attempt to make up in logical consistency.
By specifying preferences (what the agents want), technology (what the agents can produce), and institutions (the way they interact), it is possible (in principle, though challenging in practice) to solve the DSGE model to predict what is actually produced, traded, and consumed, and how these variables evolve over time in response to various shocks. In principle, it is also possible to make predictions about the effects of changing the institutional framework.
In contrast, as Robert Lucas pointed out,[4] such a prediction is unlikely to be valid in traditional macroeconometric forecasting models, since those models are based on observed past correlations between macroeconomic variables. These correlations can be expected to change when new policies are introduced, invalidating predictions based on past observations.
Given the difficulty of constructing accurate DSGE models, most central banks still rely on traditional macroeconometric models for short-term forecasting. However, the effects of alternative policies are increasingly studied using DSGE methods. Since DSGE models are constructed on the basis of assumptions about agents' preferences, it is possible to ask whether the policies considered are Pareto optimal, which is a state of allocation of resources in which it is impossible to make any one individual better off without making at least one individual worse off, or how well they satisfy some other social welfare criterion derived from preferences (Woodford, 2003, p. 12).
At present two competing schools of thought form the bulk of DSGE modeling.[5]
The European Central Bank (ECB) has developed a DSGE model, often called the Smets–Wouters model,[10][11] which it uses to analyze the economy of the Eurozone as a whole (in other words, the model does not analyze individual European countries separately).[12] The model is intended as an alternative to the Area-Wide Model (AWM), a more traditional empirical forecasting model which the ECB has been using for several years[when?].
The equations in the Smets-Wouters model describe the choices of three types of decision makers: households, who choose consumption and hours worked optimally, under a budget constraint; firms, who decide how much labor and capital to employ; and the central bank, which controls monetary policy. The parameters in the equations were estimated using Bayesian statistical techniques so that the model approximately describes the dynamics of GDP, consumption, investment, prices, wages, employment, and interest rates in the Eurozone economy. In order to accurately reproduce the sluggish behavior of some of these variables, the model incorporates several types of frictions that slow down adjustment to shocks, including sticky prices and wages, and adjustment costs in investment.
Willem Buiter, Chief Economist at Citigroup, has argued that DSGE models rely excessively on an assumption of complete markets, and are unable to describe the highly nonlinear dynamics of economic fluctuations, making training in 'state-of-the-art' macroeconomic modeling "a privately and socially costly waste of time and resources".[13]
N. Gregory Mankiw, regarded as one of the founders of New Keynesian DSGE modeling, has also argued that
Michael Woodford, replying to Mankiw, argues that DSGE models are commonly used by central banks today, and have strongly influenced policy makers like Ben Bernanke. However, he argues that what is learned from DSGE models is not so different from traditional Keynesian analysis:
Narayana Kocherlakota, President of the Federal Reserve Bank of Minneapolis, acknowledges that DSGE models were not very useful for analyzing the financial crisis of 2007-2010.[16] Nonetheless, he argues that the applicability of these models is improving, and that there is growing consensus among macroeconomists that DSGE models need to incorporate both price stickiness and financial market frictions.
The United States Congress hosted hearings on macroeconomic modeling methods on July 20, 2010, to investigate why macroeconomists failed to foresee the financial crisis of 2007-2010. Robert Solow blasted DSGE models currently in use:
V.V. Chari pointed out, however, that state-of-the-art DSGE models are more sophisticated than their critics suppose:
Chari also argued that current DSGE models frequently incorporate frictional unemployment, financial market imperfections, and sticky prices and wages, and therefore imply that the macroeconomy behaves in a suboptimal way which monetary and fiscal policy may be able to improve.[18]
Commenting on the Congressional session, The Economist asked whether agent-based models might better predict financial crises than DSGE models.[19]
Noah Smith, economics professor and author of the blog 'noahpinion',[20] observed that "DSGE fails the market test." That is, financial modelers who would benefit directly from superior market returns uniformly do not use DSGE models, thus strongly suggesting that DSGE models are not useful for macroeconomic prediction.[21]
Paul Romer criticized the "mathiness" of these models.<ref>Paul Romer, “Mathiness in the Theory of Economic Growth,” American Economic Review: Papers & Proceedings 105 (2015), 5: 89-93.<ref> Joseph Stiglitz, describes the staggering shortcomings of the fantasy world these models create this way: “the heart of the failure [of macroeconomics] were the wrong microfoundations, which failed to incorporate key aspects of economic behaviour.”<ref>Joseph Stiglitz, “Where Modern Macroeconomics Went Wrong,” National Bureau of Economic Research, Working Paper No. 23795, September 2017; published in Oxford Review of Economic Policy, 34 (2018), 1-2: 70-106.<ref> Others suggested that these models ‘may have set back by decades serious investigations of aggregate economic behaviour and economic polity-relevant understanding. It was a privately and socially costly waste of time and resources.”<ref>Willem Buiter, “The unfortunate uselessness of most ‘state of the art” academic monetary economics,” Vox, CEPR’s Policy Portal, March 6, 2009; David Hendry and John Muellbauer, “The future of macroeconomics: macro theory and models at the Bank of England,” Oxford Review of Economic Policy, 34 (2018), 1-2: 287-328.<ref> As Stiglitz suggests, they failed to incorporate, “insights from information economics and behavioural economics. Inadequate modelling of the financial sector meant they [the models] were ill-suited for predicting or responding to a financial crisis; and a reliance on representative agent models meant they were ill-suited for analysing either the role of distribution in fluctuations and crises or the consequences of fluctuations on inequality.” Another reflection on the state of macroeconomics put it this way: “it is as if the information economics revolution, for which George Akerlof, Michael Spence and Joe Stiglitz shared the Nobel Prize in 2001, had not occurred. The combination of assumptions, when coupled with the trivialisation of risk and uncertainty… render money, credit and asset prices largely irrelevant… [they] typically ignore inconvenient truths.”<ref>John Muellbauer, “Household Decisions, Credit Markets and the Macroeconomy: Implications for the Design of Central Bank Models,” Bank for International Settlements Discussion Paper 306, March 2010. <ref>